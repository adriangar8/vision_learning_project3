{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f28f1b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'v2' from 'torchvision.transforms' (c:\\Users\\polme\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\polme\\OneDrive - UAB\\Escritorio\\Universitat\\3r Carrera\\1r Semestre\\Vision and Learning\\Challenge 3\\Baseline Model and Metrics.ipynb Celda 1\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'v2' from 'torchvision.transforms' (c:\\Users\\polme\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from transformers import ResNetModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchvision in c:\\users\\polme\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\polme\\anaconda3\\lib\\site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\polme\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\polme\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\polme\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b723e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "base_path = 'archive/'\n",
    "img_path = f'{base_path}Images/'\n",
    "cap_path = f'{base_path}captions.txt'\n",
    "\n",
    "data = pd.read_csv(cap_path)\n",
    "partitions = np.load('flickr8k_partitions.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b09b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['<SOS>', '<EOS>', '<PAD>', ' ', '!', '\"', '#', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "NUM_CHAR = len(chars)\n",
    "idx2char = {k: v for k, v in enumerate(chars)}\n",
    "char2idx = {v: k for k, v in enumerate(chars)}\n",
    "\n",
    "TEXT_MAX_LEN = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff6590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, partition):\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        self.num_captions = 5\n",
    "        self.max_len = TEXT_MAX_LEN\n",
    "        self.img_proc = torch.nn.Sequential([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224), antialias=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.num_captions*self.partition[idx]\n",
    "        item = self.data.iloc[real_idx: real_idx+self.num_captions]\n",
    "        ## image processing\n",
    "        img_name = item.image.reset_index(drop=True)[0]\n",
    "        img = Image.open(f'{img_path}{img_name}').convert('RGB')\n",
    "        img = self.img_proc(img)\n",
    "    \n",
    "        ## caption processing\n",
    "        caption = item.caption.reset_index(drop=True)[random.choice(list(range(self.num_captions)))]\n",
    "        cap_list = list(caption)\n",
    "        final_list = [chars[0]]\n",
    "        final_list.extend(cap_list)\n",
    "        final_list.extend([chars[1]])\n",
    "        gap = self.max_len - len(final_list)\n",
    "        final_list.extend([chars[2]]*gap)\n",
    "        cap_idx = [char2idx[i] for i in final_list]\n",
    "        return img, cap_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img)\n",
    "        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0) # 1, batch, 512\n",
    "        start = torch.tensor(char2idx['<SOS>']).to(DEVICE)\n",
    "        start_embed = self.embed(start) # 512\n",
    "        start_embeds = start_embed.repeat(batch_size, 1).unsqueeze(0) # 1, batch, 512\n",
    "        inp = start_embeds\n",
    "        hidden = feat\n",
    "        for t in range(TEXT_MAX_LEN-1): # rm <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            inp = torch.cat((inp, out[-1:]), dim=0) # N, batch, 512\n",
    "    \n",
    "        res = inp.permute(1, 0, 2) # batch, seq, 512\n",
    "        res = self.proj(res) # batch, seq, 80\n",
    "        res = res.permute(0, 2, 1) # batch, 80, seq\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82acf476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-18 were not used when initializing ResNetModel: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3552, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''A simple example to calculate loss of a single batch (size 2)'''\n",
    "dataset = Data(data, partitions['train'][:10])\n",
    "img1, caption1 = next(iter(dataset))\n",
    "img2, caption2 = next(iter(dataset))\n",
    "caption1 = torch.tensor(caption1)\n",
    "caption2 = torch.tensor(caption2)\n",
    "img = torch.cat((img1.unsqueeze(0), img2.unsqueeze(0)))\n",
    "caption = torch.cat((caption1.unsqueeze(0), caption2.unsqueeze(0)))\n",
    "img, caption = img.to(DEVICE), caption.to(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "pred = model(img)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "loss = crit(pred, caption)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3606f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\polme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\polme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\polme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.5946035575013605,\n",
       "  'precisions': [0.875, 0.7142857142857143, 0.5, 0.4],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 1.0,\n",
       "  'translation_length': 8,\n",
       "  'reference_length': 8},\n",
       " {'rouge1': 0.8571428571428571,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.8571428571428571,\n",
       "  'rougeLsum': 0.8571428571428571},\n",
       " {'meteor': 0.864795918367347})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''metrics'''\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "reference = [['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .']]\n",
    "prediction = ['A girl goes into a wooden building .']\n",
    "\n",
    "res_b = bleu.compute(predictions=prediction, references=reference)\n",
    "res_r = rouge.compute(predictions=prediction, references=reference)\n",
    "res_m = meteor.compute(predictions=prediction, references=reference)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7595f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.4723665527410147,\n",
       "  'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       "  'brevity_penalty': 0.4723665527410147,\n",
       "  'length_ratio': 0.5714285714285714,\n",
       "  'translation_length': 4,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.7272727272727273,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.7272727272727273,\n",
       "  'rougeLsum': 0.7272727272727273},\n",
       " {'meteor': 0.5923507462686567})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is running']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe24da77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 1.0, 1.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.5, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.44612794612794615})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1afd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 0.5, 0.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.25, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.3872053872053872},\n",
       " {'meteor': 0.45454545454545453})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "res_m_sin = meteor.compute(predictions=pred1, references=ref, gamma=0) # no penalty by setting gamma to 0\n",
    "\n",
    "res_b, res_r, res_m, res_m_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bac05",
   "metadata": {},
   "source": [
    "Final metric we use for challenge 3: BLEU1, BLEU2, ROUGE-L, METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5433823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLEU-1:26.4%, BLEU2:18.6%, ROUGE-L:60.0%, METEOR:38.7%'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "bleu1 = bleu.compute(predictions=pred1, references=ref, max_order=1)\n",
    "bleu2 = bleu.compute(predictions=pred1, references=ref, max_order=2)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "f\"BLEU-1:{bleu1['bleu']*100:.1f}%, BLEU2:{bleu2['bleu']*100:.1f}%, ROUGE-L:{res_r['rougeL']*100:.1f}%, METEOR:{res_m['meteor']*100:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf5d9d",
   "metadata": {},
   "source": [
    "Now it is your turn! Try to finish the code below to run the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9835fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(EPOCHS):\n",
    "    data_train = Data(data, partitions['train'][:10])\n",
    "    data_valid = Data(data, partitions['valid'][:2])\n",
    "    data_test = Data(data, partitions['test'][:5])\n",
    "    dataloader_train = DataLoader(data_train, batch_size=8) # '''write a proper dataloader, same for valid and test'''\n",
    "    dataloader_valid = DataLoader(data_valid, batch_size=4)\n",
    "    dataloader_test = DataLoader(data_test, batch_size=2)\n",
    "    model = Model().to(DEVICE)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=10**-5) # '''choose a proper optimizer'''\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    metric = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss, res = train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n",
    "        print(f'train loss: {loss:.2f}, metric: {res:.2f}, epoch: {epoch}')\n",
    "        loss_v, res_v = eval_epoch(model, crit, metric, dataloader_valid)\n",
    "        print(f'valid loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    loss_t, res_t = eval_epoch(model, crit, metric, dataloader_test)\n",
    "    print(f'test loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    \n",
    "def train_one_epoch(model, optimizer, crit, metric, dataloader):\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, captions) in enumerate(dataloader):\n",
    "        print(batch_idx)\n",
    "        print(captions)\n",
    "        imgs, captions = imgs.to(DEVICE), torch.Tensor(captions).to(DEVICE)\n",
    "        res = model(imgs)\n",
    "        print(res.shape, captions.shape)\n",
    "        loss = crit(res, caption)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    '''finish the code'''\n",
    "    return loss, res\n",
    "\n",
    "def eval_epoch(model, crit, metric, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img, caption in dataloader:\n",
    "            img, caption = img.to(DEVICE), torch.Tensor(caption).to(DEVICE)\n",
    "            res = model(img)\n",
    "            loss = crit(res, caption)\n",
    "    '''finish the code'''\n",
    "    return loss, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-18 were not used when initializing ResNetModel: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([28, 28, 28, 28, 28, 47, 28, 47]), tensor([ 3,  3,  3,  3,  3, 61,  3, 61]), tensor([55, 65, 76, 57, 65, 58, 64, 58]), tensor([71, 62, 61, 68, 62,  3, 62,  3]), tensor([68, 73, 62, 60, 73, 73, 57, 60]), tensor([76, 73, 73,  3, 73, 76,  3, 62]), tensor([67, 65, 58, 56, 65, 68, 76, 71]), tensor([ 3, 58,  3, 61, 58,  3, 62, 65]), tensor([57,  3, 57, 58,  3, 73, 73,  3]), tensor([68, 60, 68, 76, 60, 54, 61, 76]), tensor([60, 62, 60, 72, 62, 67,  3, 62]), tensor([ 3, 71,  3,  3, 71,  3, 54, 73]), tensor([71, 65, 55, 54, 65, 56,  3, 61]), tensor([74,  3, 68, 67,  3, 68, 55,  3]), tensor([67, 62, 74,  3, 76, 65, 71, 60]), tensor([72, 67, 67, 68, 54, 68, 68, 65]), tensor([ 3,  3, 57, 55, 65, 71, 64, 54]), tensor([73, 54, 62, 63, 64, 58, 58, 72]), tensor([61,  3, 67, 58, 62, 57, 67, 72]), tensor([71, 55, 60, 56, 67,  3,  3, 58]), tensor([68, 65,  3, 73, 60, 57, 54, 72]), tensor([74, 74, 73,  3,  3, 68, 71,  3]), tensor([60, 58, 61, 54, 68, 60, 66, 68]), tensor([61,  3, 71,  3, 67, 72,  3, 67]), tensor([ 3, 54, 68, 76,  3,  3, 54,  3]), tensor([73, 67, 74, 68, 73, 54, 72, 61]), tensor([61, 57, 60, 66, 61, 71, 65, 54]), tensor([58,  3, 61, 54, 58, 58, 58, 72]), tensor([ 3, 69,  3, 67,  3,  3, 58,  3]), tensor([60, 62, 72,  3, 60, 62, 69, 54]), tensor([71, 67, 67, 62, 71, 67,  3,  3]), tensor([54, 64, 68, 72, 58,  3, 62, 55]), tensor([72,  3, 76,  3, 58, 54, 67, 74]), tensor([72, 68,  1, 61, 67,  3,  3, 60]), tensor([ 3, 74,  2, 68,  3, 59, 61,  3]), tensor([61, 73,  2, 65, 60, 62, 62, 68]), tensor([68, 59,  2, 57, 71, 58, 72, 67]), tensor([65, 62,  2, 62, 54, 65,  3,  3]), tensor([57, 73,  2, 67, 72, 57, 55, 61]), tensor([62,  3,  2, 60, 72,  3, 58, 58]), tensor([67, 62,  2,  3,  3, 11, 57, 71]), tensor([60, 72,  2, 68, 62,  3,  1,  3]), tensor([ 3,  3,  2, 67, 67, 54,  2, 71]), tensor([72, 76,  2,  3,  3, 67,  2, 62]), tensor([68, 54,  2, 73, 59, 57,  2, 60]), tensor([66, 65,  2, 61, 71,  3,  2, 61]), tensor([58, 64,  2, 58, 68, 68,  2, 73]), tensor([73, 62,  2,  3, 67, 67,  2,  3]), tensor([61, 67,  2, 71, 73, 58,  2, 69]), tensor([62, 60,  2, 68,  3,  3,  2, 68]), tensor([67,  3,  2, 56, 68, 62,  2, 62]), tensor([60, 68,  2, 64, 59, 72,  2, 67]), tensor([ 3, 67,  2, 78,  3,  3,  2, 73]), tensor([62,  3,  2,  3, 54, 63,  2, 58]), tensor([67, 54,  2, 54,  3, 74,  2, 71]), tensor([ 3,  3,  2, 67, 55, 66,  2,  3]), tensor([62, 55,  2, 57, 62, 69,  2, 59]), tensor([73, 54,  2,  3, 60, 62,  2, 62]), tensor([72, 65,  2, 72,  3, 67,  2, 67]), tensor([ 3, 54,  2, 54, 72, 60,  2, 60]), tensor([66, 67,  2, 67, 73,  3,  2, 58]), tensor([68, 56,  2, 57, 68, 62,  2, 71]), tensor([74, 58,  2, 78, 67, 67,  2,  3]), tensor([73,  3,  2,  3, 58,  3,  2, 13]), tensor([61, 55,  2, 55,  3, 73,  2,  1]), tensor([ 3, 58,  2, 58, 13, 61,  2,  2]), tensor([13, 54,  2, 54,  1, 58,  2,  2]), tensor([ 1, 66,  2, 56,  2,  3,  2,  2]), tensor([ 2,  3,  2, 61,  2, 54,  2,  2]), tensor([ 2, 76,  2,  3,  2, 62,  2,  2]), tensor([ 2, 62,  2, 13,  2, 71,  2,  2]), tensor([ 2, 73,  2,  1,  2,  3,  2,  2]), tensor([ 2, 61,  2,  2,  2, 13,  2,  2]), tensor([2, 3, 2, 2, 2, 1, 2, 2]), tensor([ 2, 73,  2,  2,  2,  2,  2,  2]), tensor([ 2, 61,  2,  2,  2,  2,  2,  2]), tensor([ 2, 58,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 54,  2,  2,  2,  2,  2,  2]), tensor([ 2, 62,  2,  2,  2,  2,  2,  2]), tensor([ 2, 57,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 68,  2,  2,  2,  2,  2,  2]), tensor([ 2, 59,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 76,  2,  2,  2,  2,  2,  2]), tensor([ 2, 68,  2,  2,  2,  2,  2,  2]), tensor([ 2, 66,  2,  2,  2,  2,  2,  2]), tensor([ 2, 54,  2,  2,  2,  2,  2,  2]), tensor([ 2, 67,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 72,  2,  2,  2,  2,  2,  2]), tensor([ 2, 73,  2,  2,  2,  2,  2,  2]), tensor([ 2, 54,  2,  2,  2,  2,  2,  2]), tensor([ 2, 67,  2,  2,  2,  2,  2,  2]), tensor([ 2, 57,  2,  2,  2,  2,  2,  2]), tensor([ 2, 62,  2,  2,  2,  2,  2,  2]), tensor([ 2, 67,  2,  2,  2,  2,  2,  2]), tensor([ 2, 60,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 73,  2,  2,  2,  2,  2,  2]), tensor([ 2, 68,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 61,  2,  2,  2,  2,  2,  2]), tensor([ 2, 58,  2,  2,  2,  2,  2,  2]), tensor([ 2, 71,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 65,  2,  2,  2,  2,  2,  2]), tensor([ 2, 58,  2,  2,  2,  2,  2,  2]), tensor([ 2, 59,  2,  2,  2,  2,  2,  2]), tensor([ 2, 73,  2,  2,  2,  2,  2,  2]), tensor([2, 3, 2, 2, 2, 2, 2, 2]), tensor([ 2, 13,  2,  2,  2,  2,  2,  2]), tensor([2, 1, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\polme\\OneDrive - UAB\\Escritorio\\Universitat\\3r Carrera\\1r Semestre\\Vision and Learning\\Challenge 3\\Baseline Model and Metrics.ipynb Celda 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\polme\\OneDrive - UAB\\Escritorio\\Universitat\\3r Carrera\\1r Semestre\\Vision and Learning\\Challenge 3\\Baseline Model and Metrics.ipynb Celda 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m metric \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss, res \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, metric: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss_v, res_v \u001b[39m=\u001b[39m eval_epoch(model, crit, metric, dataloader_valid)\n",
      "\u001b[1;32mc:\\Users\\polme\\OneDrive - UAB\\Escritorio\\Universitat\\3r Carrera\\1r Semestre\\Vision and Learning\\Challenge 3\\Baseline Model and Metrics.ipynb Celda 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(batch_idx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(captions)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m imgs, captions \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(DEVICE), torch\u001b[39m.\u001b[39;49mTensor(captions)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m res \u001b[39m=\u001b[39m model(imgs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/polme/OneDrive%20-%20UAB/Escritorio/Universitat/3r%20Carrera/1r%20Semestre/Vision%20and%20Learning/Challenge%203/Baseline%20Model%20and%20Metrics.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(res\u001b[39m.\u001b[39mshape, captions\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
